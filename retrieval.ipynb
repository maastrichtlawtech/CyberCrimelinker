{"cells":[{"cell_type":"markdown","id":"9a711713-473a-4625-9356-8b4d51da26d6","metadata":{"id":"9a711713-473a-4625-9356-8b4d51da26d6"},"source":["# **Installing the packages**"]},{"cell_type":"code","execution_count":null,"id":"1d2f5223-bab9-4cae-9c47-7efd16c20b66","metadata":{"tags":[],"id":"1d2f5223-bab9-4cae-9c47-7efd16c20b66","outputId":"0930a8db-322f-4be4-ed6d-5a95954ba92b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.2)\n","Collecting plotly\n","  Obtaining dependency information for plotly from https://files.pythonhosted.org/packages/00/4e/6258fc3b26f1f7abd1b2e75b1e9e4f12f13584136e2e1549f995ff4c6b7b/plotly-5.20.0-py3-none-any.whl.metadata\n","  Downloading plotly-5.20.0-py3-none-any.whl.metadata (7.0 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0a0+32f93b1)\n","Collecting lightning\n","  Obtaining dependency information for lightning from https://files.pythonhosted.org/packages/a0/4a/b7d4f62449d940ce43d4657322a14f5718815b648f9d2b0b23a195acb646/lightning-2.2.1-py3-none-any.whl.metadata\n","  Downloading lightning-2.2.1-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n","\u001b[?25hCollecting transformers\n","  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/b6/4d/fbe6d89fde59d8107f0a02816c4ac4542a8f9a85559fdf33c68282affcc1/transformers-4.38.2-py3-none-any.whl.metadata\n","  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\n","  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Collecting tenacity>=6.2.0 (from plotly)\n","  Obtaining dependency information for tenacity>=6.2.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n","  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.2.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n","Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n","  Obtaining dependency information for lightning-utilities<2.0,>=0.8.0 from https://files.pythonhosted.org/packages/7d/84/fce34a549e2f795b3a0427e7dd40719dd4f00036e50ba58198a5a706eb75/lightning_utilities-0.10.1-py3-none-any.whl.metadata\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n","  Obtaining dependency information for torchmetrics<3.0,>=0.7.0 from https://files.pythonhosted.org/packages/cd/23/4bb4c1b78b57682a1309974a29bfdcbfa6fcf5476e698a4f0f22affa3799/torchmetrics-1.3.1-py3-none-any.whl.metadata\n","  Downloading torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n","Collecting pytorch-lightning (from lightning)\n","  Obtaining dependency information for pytorch-lightning from https://files.pythonhosted.org/packages/56/ed/192d7518b15a06452f480346eeebe1d1d4595af80687e142b2e6f18539fd/pytorch_lightning-2.2.1-py3-none-any.whl.metadata\n","  Downloading pytorch_lightning-2.2.1-py3-none-any.whl.metadata (21 kB)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n","  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/ab/28/d4b691840d73126d4c9845f8a22dad033ac872509b6d3a0d93b456eef424/huggingface_hub-0.21.4-py3-none-any.whl.metadata\n","  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.19,>=0.14 (from transformers)\n","  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/1c/5d/cf5e122ce4f1a29f165b2a69dc33d1ff30bce303343d58a54775ddba5d51/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting safetensors>=0.4.1 (from transformers)\n","  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/d0/ba/b2254fafc7f5fdc98a2fa4d5a5eeb029fbf9589ec87f2c230c3ac0a1dd53/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting pyarrow>=12.0.0 (from datasets)\n","  Obtaining dependency information for pyarrow>=12.0.0 from https://files.pythonhosted.org/packages/e1/41/b0a9bf304d47c18cb4f14cf2f5431eeb4e2259cb74f6e866904f4eb783a5/pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n","  Downloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Collecting pyarrow-hotfix (from datasets)\n","  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Collecting xxhash (from datasets)\n","  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/bc/f7/7ec7fddc92e50714ea3745631f79bd9c96424cb2702632521028e57d3a36/multiprocess-0.70.16-py310-none-any.whl.metadata\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (68.2.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Downloading plotly-5.20.0-py3-none-any.whl (15.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading lightning-2.2.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m215.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m255.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m496.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m461.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Downloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m287.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m483.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n","Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m412.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m464.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m421.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m419.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m473.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m448.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, tenacity, safetensors, pyarrow-hotfix, pyarrow, lightning-utilities, dill, plotly, multiprocess, huggingface-hub, torchmetrics, tokenizers, transformers, pytorch-lightning, datasets, lightning\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 11.0.0\n","    Uninstalling pyarrow-11.0.0:\n","      Successfully uninstalled pyarrow-11.0.0\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.7\n","    Uninstalling dill-0.3.7:\n","      Successfully uninstalled dill-0.3.7\n","Successfully installed datasets-2.18.0 dill-0.3.8 huggingface-hub-0.21.4 lightning-2.2.1 lightning-utilities-0.10.1 multiprocess-0.70.16 plotly-5.20.0 pyarrow-15.0.1 pyarrow-hotfix-0.6 pytorch-lightning-2.2.1 safetensors-0.4.2 tenacity-8.2.3 tokenizers-0.15.2 torchmetrics-1.3.1 transformers-4.38.2 xxhash-3.4.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["! pip install tqdm pandas numpy plotly scikit-learn matplotlib torch lightning transformers datasets faiss-cpu\n","# ! python -m pip install git+https://github.com/osainz59/t5-encoder"]},{"cell_type":"markdown","id":"93bdd2c2-fbf4-423d-bbeb-c7bccef5c433","metadata":{"id":"93bdd2c2-fbf4-423d-bbeb-c7bccef5c433"},"source":["# **Importing the libraries**"]},{"cell_type":"code","execution_count":null,"id":"98b27669-4f7c-4d9c-83ad-80448ff96350","metadata":{"tags":[],"id":"98b27669-4f7c-4d9c-83ad-80448ff96350","outputId":"4b5edfd7-85e0-424b-81d3-f6a66d928045"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/envs/mix/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# Mount the drive if not mounted\n","from google.colab import drive\n","drive.mount(\"/content/drive/\")\n","\n","import os\n","import random\n","from collections import Counter\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, random_split, TensorDataset\n","\n","from datasets import load_dataset\n","\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\n","\n","import lightning as L\n","import lightning.pytorch as pl\n","from lightning.pytorch import Trainer, LightningModule, LightningDataModule, seed_everything\n","from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n","from lightning.pytorch.callbacks import TQDMProgressBar\n","# from lightning.pytorch.strategies import DeepSpeedStrategy\n","from lightning.pytorch.plugins.precision import DeepSpeedPrecisionPlugin\n","\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","\n","# from deepspeed.ops.adam import DeepSpeedCPUAdam\n","\n","# import t5_encoder\n","\n","import faiss\n","\n","# import wandb\n","# wandb.login(relogin=True)\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"82e46a6c-63da-4221-9fdf-11f2b03ad522","metadata":{"id":"82e46a6c-63da-4221-9fdf-11f2b03ad522"},"source":["# **Setting seed value for reproducibility**    "]},{"cell_type":"code","execution_count":null,"id":"16e7345b-c9ff-40c5-b343-0e7dcd777ace","metadata":{"tags":[],"id":"16e7345b-c9ff-40c5-b343-0e7dcd777ace","outputId":"55bf482d-483e-49cc-c397-3782cab1990f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Seed set to 111\n"]},{"data":{"text/plain":["111"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["seed = 111\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(seed)\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n","seed_everything(seed)"]},{"cell_type":"markdown","id":"3c87d8e8-2709-4c34-99fa-7086b6d01bf6","metadata":{"id":"3c87d8e8-2709-4c34-99fa-7086b6d01bf6"},"source":["# **Creating DataModule**"]},{"cell_type":"code","execution_count":null,"id":"e2adb522-06f5-4949-8f06-f65f7e20c619","metadata":{"tags":[],"id":"e2adb522-06f5-4949-8f06-f65f7e20c619"},"outputs":[],"source":["class contextualizedClassifierDataModule(pl.LightningDataModule):\n","    def __init__(self, tokenizer_name_or_path, batch_size=32):\n","        super().__init__()\n","\n","        self.tokenizer_name_or_path = tokenizer_name_or_path\n","        self.batch_size = batch_size\n","\n","        # Handling the padding token in distilgpt2 by substituting it with eos_token_id\n","        if self.tokenizer_name_or_path == \"distilgpt2\":\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name_or_path, use_fast=True)\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","        else:\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name_or_path, use_fast=True)\n","\n","    def setup(self, stage=None):\n","        # Load the dataset into a pandas dataframe.\n","        # Load the data from a CSV file\n","        # Pre-processing data\n","        df = pd.read_csv(\"data/Agora.csv\", encoding='ISO-8859-1')\n","        # df = pd.read_csv(\"/content/drive/MyDrive/AA-Tutorial/data/Agora.csv\", encoding='ISO-8859-1')\n","        # Renaming all the features of the dataframe\n","        df = df.rename(str.strip, axis='columns')\n","        # Merging the Item and Item Description using a [SEP] token\n","        separator = ' [SEP] '\n","        df['TEXT'] = df.apply(lambda row: f\"{row['Item']}{separator}{row['Item Description']}\", axis=1)\n","        # dropping Unncessary columns\n","        df.drop(columns=[\"Item\", \"Item Description\", \"Category\", \"Price\", \"Origin\", \"Destination\", \"Rating\", \"Remarks\"], inplace=True)\n","        # Assuming that vendors Amsterdam100 and amsterdam100 are the same vendors\n","        df.Vendor = df.Vendor.apply(lambda x: x.lower())\n","\n","        # Due to the extensive time required to train on over 100K+ samples, we have decided to limit our analysis to a subset of 5K samples.\n","        # To get these samples, we look into vendors that have 5+ advertisements and then allocate all the vendors that have less than 5 ads into a new class, \"others\".\n","        df = df.iloc[:5000]\n","        # Assigning a vendor ID to \"others\" class\n","        # vendors_dict[\"others\"] = len(vendors_dict) + 1\n","        # Calculate advertisement frequency for each vendor\n","        ad_freq = df['Vendor'].value_counts()\n","        # Filter vendors with ad frequency less than 5\n","        vendors_to_replace = ad_freq[ad_freq < 5].index\n","        # Update DataFrame: Replace vendor names with 'others' where ad frequency is less than 5\n","        df['Vendor'] = df['Vendor'].apply(lambda x: 'others' if x in vendors_to_replace else x)\n","\n","        # Assigning vendor IDs to vendor handles using a dictionary comprehension.\n","        # This approach eliminates the need for checking if a vendor already exists in the dictionary,\n","        # as each unique vendor will be processed once. The enumerate function provides a counter (idx),\n","        # which is used to assign IDs, starting from 1 for the first vendor.\n","        vendors_dict = {vendor: idx for idx, vendor in enumerate(df.Vendor.unique())}\n","\n","        # Updating the 'Vendor' column in the DataFrame to reflect the vendor IDs.\n","        # The 'map' function is used to replace each vendor handle with its corresponding vendor ID\n","        # based on the 'vendor_to_idx_dict'. This operation is vectorized and efficient.\n","        df['Vendor'] = df['Vendor'].map(vendors_dict)\n","\n","        text = df.TEXT.values.tolist()\n","        vendors = df.Vendor.values.tolist()\n","\n","        # Tokenizing the data with padding and truncation\n","        encodings = self.tokenizer(text, add_special_tokens=True, max_length=512, padding='max_length', return_token_type_ids=False, truncation=True,\n","                                   return_attention_mask=True, return_tensors='pt')\n","\n","        # Convert the lists into tensors.\n","        input_ids = encodings['input_ids']\n","        attention_mask = encodings['attention_mask']\n","        labels = torch.tensor(vendors)\n","\n","        # Combine the inputs into a TensorDataset.\n","        dataset = TensorDataset(input_ids, attention_mask, labels)\n","\n","        # Getting an 0.75-0.05-0.20 split for training-val-test dataset\n","        train_dataset, test_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(1111))\n","        train_dataset, val_dataset = random_split(train_dataset, [0.95, 0.05], generator=torch.Generator().manual_seed(1111))\n","\n","        self.train_dataset = train_dataset\n","        self.test_dataset = test_dataset\n","        self.val_dataset = val_dataset\n","\n","    # Returning the pytorch-lightning default training DataLoader\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, sampler=RandomSampler(self.train_dataset), batch_size=self.batch_size)\n","\n","    # Returning the pytorch-lightning default val DataLoader\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n","\n","    # Returning the pytorch-lightning default test DataLoader\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size)"]},{"cell_type":"markdown","id":"bae14bac-e580-49bd-9755-0930e8daa853","metadata":{"id":"bae14bac-e580-49bd-9755-0930e8daa853"},"source":["# **Initializing Lightning Model Module**"]},{"cell_type":"code","execution_count":null,"id":"4f541536-b81a-49f0-bf84-1aece5cc19c6","metadata":{"tags":[],"id":"4f541536-b81a-49f0-bf84-1aece5cc19c6"},"outputs":[],"source":["class ClassifierModel(pl.LightningModule):\n","    def __init__(self, learning_rate, adam_epsilon, weight_decay, model_name_or_path, num_classes, num_training_steps, warmup_steps):\n","        super().__init__()\n","\n","        self.save_hyperparameters()\n","        self.hparams.learning_rate = learning_rate\n","        self.hparams.eps = adam_epsilon\n","        self.hparams.weight_decay = weight_decay\n","        self.hparams.model_name_or_path = model_name_or_path\n","        self.hparams.num_classes = num_classes\n","        self.hparams.num_training_steps = num_training_steps\n","        self.hparams.warmup_steps = warmup_steps\n","\n","        # freeze\n","        self._frozen = False\n","\n","        # Handling the padding token in distilgpt2 by substituting it with eos_token_id\n","        if self.hparams.model_name_or_path == \"distilgpt2\":\n","            config = AutoConfig.from_pretrained(self.hparams.model_name_or_path, num_labels=self.hparams.num_classes, output_attentions=True, output_hidden_states=True)\n","            self.model = AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name_or_path, config=config)\n","            self.model.config.pad_token_id = self.model.config.eos_token_id\n","        else:\n","            config = AutoConfig.from_pretrained(self.hparams.model_name_or_path, num_labels=self.hparams.num_classes, output_attentions=True, output_hidden_states=True)\n","            self.model = AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name_or_path, config=config)\n","\n","    def forward(self, batch):\n","        # The batch contains the input_ids, the input_put_mask and the labels (for training)\n","        input_ids = batch[0]\n","        input_mask = batch[1]\n","        labels = batch[2]\n","        outputs = self.model(input_ids, attention_mask=input_mask, labels=labels)\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        return loss, logits, outputs[\"hidden_states\"], outputs[\"attentions\"]\n","\n","    def training_step(self, batch, batch_nb):\n","        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n","        # class stipulates you to overwrite. This we do here, by virtue of this definition\n","        outputs = self(batch)  # self refers to the model, which in turn acceses the forward method\n","        train_loss = outputs[0]\n","        self.log_dict({\"train_loss\": train_loss}, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        return train_loss\n","        # the training_step method expects a dictionary, which should at least contain the loss\n","\n","    def validation_step(self, batch, batch_nb):\n","        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n","        # class  wants you to overwrite, in case you want to do validation. This we do here, by virtue of this definition.\n","\n","        outputs = self(batch)\n","        # self refers to the model, which in turn accesses the forward method\n","\n","        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n","        # model training has achieved \"in real terms\".\n","        val_loss = outputs[0]\n","        logits = outputs[1]\n","        labels = batch[2]\n","\n","        # Evaluating the performance\n","        predictions = torch.argmax(logits, dim=1)\n","        balanced_accuracy = balanced_accuracy_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), adjusted=True)\n","        macro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='macro')\n","        micro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='micro')\n","        weighted_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='weighted')\n","\n","        self.log_dict({\"val_loss\": val_loss, 'accuracy': balanced_accuracy, 'macro-F1': macro_accuracy, 'micro-F1': micro_accuracy, 'weighted-F1':weighted_accuracy},\n","                      on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        return val_loss\n","\n","    def test_step(self, batch, batch_nb):\n","        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n","        # class  wants you to overwrite, in case you want to do test. This we do here, by virtue of this definition.\n","\n","        outputs = self(batch)\n","        # self refers to the model, which in turn accesses the forward method\n","\n","        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n","        # model training has achieved \"in real terms\".\n","        test_loss = outputs[0]\n","        logits = outputs[1]\n","        labels = batch[2]\n","\n","        # Evaluating the performance\n","        predictions = torch.argmax(logits, dim=1)\n","        balanced_accuracy = balanced_accuracy_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), adjusted=True)\n","        macro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='macro')\n","        micro_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='micro')\n","        weighted_accuracy = f1_score(labels.detach().cpu().numpy(), predictions.detach().cpu().numpy(), average='weighted')\n","\n","        self.log_dict({\"test_loss\": test_loss, 'accuracy': balanced_accuracy, 'macro-F1': macro_accuracy, 'micro-F1': micro_accuracy, 'weighted-F1':weighted_accuracy},\n","                      on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","    def predict_step(self, batch, batch_nb):\n","        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n","        # class  wants you to overwrite, in case you want to do validation. This we do here, by virtue of this definition.\n","\n","        outputs = self(batch)\n","        # self refers to the model, which in turn accesses the forward method\n","\n","        # Apart from the validation loss, we also want to track validation accuracy  to get an idea, what the\n","        # model training has achieved \"in real terms\".\n","        val_loss = outputs[0]\n","        logits = outputs[1]\n","        labels = batch[2]\n","\n","        predictions = torch.argmax(logits, dim=1)\n","        return predictions.detach().cpu().numpy()\n","\n","    def configure_optimizers(self):\n","        # The configure_optimizers is a (virtual) method, specified in the interface, that the\n","        # pl.LightningModule class wants you to overwrite.\n","\n","        # In this case we define that some parameters are optimized in a different way than others. In\n","        # particular we single out parameters that have 'bias', 'LayerNorm.weight' in their names. For those\n","        # we do not use an optimization technique called weight decay.\n","\n","        no_decay = ['bias', 'LayerNorm.weight']\n","\n","        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay':self.hparams.weight_decay},\n","                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.eps)\n","        # optimizer = DeepSpeedCPUAdam(optimizer_grouped_parameters, adamw_mode=True, lr=self.hparams.learning_rate, betas=(0.9, 0.999), eps=self.hparams.eps)\n","\n","        # We also use a scheduler that is supplied by transformers.\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.num_training_steps)\n","        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n","\n","        return [optimizer], [scheduler]\n","\n","    def freeze(self) -> None:\n","        # freeze all layers, except the final classifier layers\n","        for name, param in self.model.named_parameters():\n","            if 'classifier' not in name:  # classifier layer\n","                param.requires_grad = False\n","\n","        self._frozen = True\n","\n","    def unfreeze(self) -> None:\n","        if self._frozen:\n","            for name, param in self.model.named_parameters():\n","                if 'classifier' not in name:  # classifier layer\n","                    param.requires_grad = True\n","\n","        self._frozen = False\n","\n","    def train_epoch_start(self):\n","        \"\"\"pytorch lightning hook\"\"\"\n","        if self.current_epoch < self.hparams.nr_frozen_epochs:\n","            self.freeze()\n","\n","        if self.current_epoch >= self.hparams.nr_frozen_epochs:\n","            self.unfreeze()"]},{"cell_type":"markdown","id":"cb9b797e-462e-4f9a-9e21-8e5b0ac0a50e","metadata":{"id":"cb9b797e-462e-4f9a-9e21-8e5b0ac0a50e"},"source":["# **Helper functions**"]},{"cell_type":"code","execution_count":null,"id":"4a6a392d-661e-497b-894d-515c3c2bef58","metadata":{"tags":[],"id":"4a6a392d-661e-497b-894d-515c3c2bef58"},"outputs":[],"source":["def train_model(tokenizer_name, model_name):\n","    # Loading the datamodule\n","    dm = contextualizedClassifierDataModule(tokenizer_name_or_path=tokenizer_name)\n","    dm.setup()\n","\n","    # Change the number of classes as you increase the size of the dataset\n","    num_classes = 153\n","    nb_epochs = 10\n","\n","    num_training_steps = len(dm.train_dataloader()) * nb_epochs\n","    # Setting the warmup steps to 1/10th the size of training data\n","    warmup_steps = int(len(dm.train_dataloader()) * 10/100)\n","\n","    # Loading the model\n","    model = ClassifierModel(learning_rate=0.0001, adam_epsilon=float(1e-6), weight_decay=0.01, model_name_or_path=model_name, num_classes=num_classes,\n","                        num_training_steps=num_training_steps, warmup_steps=warmup_steps)\n","\n","    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=5, verbose=False, mode=\"min\")\n","    # wandb_logger = WandbLogger(save_dir=\"logs\", name=model_name, project=\"AA-Tutorials\")\n","\n","    # %% Setting up the trainer\n","    # Unfortunately the lr_finder functionality doesn't support DeepSpeedStrategy yet, therefore we will set up our trainer twice. Once to find the suitable\n","    # learning rate and secondly to train our model.\n","    trainer = L.Trainer(max_epochs=nb_epochs,\n","                  accelerator=\"gpu\",\n","                  devices=1 if torch.cuda.is_available() else None,\n","                  fast_dev_run=False,\n","                  accumulate_grad_batches = 1, # To run the backward step after n batches, helps to increase the batch size\n","                  benchmark = True, # Fastens the training process\n","                  deterministic=True, # Ensures reproducibility\n","                  limit_train_batches=1.0, # trains on 10% of the data,\n","                  check_val_every_n_epoch = 10, # run val loop every 1 training epochs\n","                  callbacks=[early_stop_callback], # Enables model checkpoint and early stopping\n","                  # logger = wandb_logger,\n","                  precision='16-mixed') # Mixed Precision system\n","\n","    # Training model\n","    trainer.fit(model, dm)\n","    # Evaluating model\n","    trainer.test(model=model, dataloaders=dm.test_dataloader())\n","\n","    return trainer, model, dm"]},{"cell_type":"markdown","id":"f1262295-7f10-4d9a-ae75-a12fbbe19f6e","metadata":{"id":"f1262295-7f10-4d9a-ae75-a12fbbe19f6e"},"source":["[DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base)"]},{"cell_type":"code","execution_count":null,"id":"4bcdc976-9ac2-4c36-9328-c17391cdf9a6","metadata":{"tags":[],"id":"4bcdc976-9ac2-4c36-9328-c17391cdf9a6","outputId":"b2f2013b-589c-4078-c167-d0477bbe44b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name  | Type                             | Params\n","-----------------------------------------------------------\n","0 | model | RobertaForSequenceClassification | 82.2 M\n","-----------------------------------------------------------\n","82.2 M    Trainable params\n","0         Non-trainable params\n","82.2 M    Total params\n","328.944   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 119/119 [00:10<00:00, 11.13it/s, v_num=5, train_loss=0.049] \n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14%|█▍        | 1/7 [00:00<00:00, 24.90it/s]\u001b[A\n","Validation DataLoader 0:  29%|██▊       | 2/7 [00:00<00:00, 25.50it/s]\u001b[A\n","Validation DataLoader 0:  43%|████▎     | 3/7 [00:00<00:00, 25.69it/s]\u001b[A\n","Validation DataLoader 0:  57%|█████▋    | 4/7 [00:00<00:00, 25.77it/s]\u001b[A\n","Validation DataLoader 0:  71%|███████▏  | 5/7 [00:00<00:00, 25.86it/s]\u001b[A\n","Validation DataLoader 0:  86%|████████▌ | 6/7 [00:00<00:00, 25.87it/s]\u001b[A\n","Validation DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 28.05it/s]\u001b[A\n","Epoch 9: 100%|██████████| 119/119 [00:12<00:00,  9.71it/s, v_num=5, train_loss=0.0445, val_loss=0.765, accuracy=0.818, macro-F1=0.770, micro-F1=0.830, weighted-F1=0.819]"]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=10` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 119/119 [00:12<00:00,  9.69it/s, v_num=5, train_loss=0.0445, val_loss=0.765, accuracy=0.818, macro-F1=0.770, micro-F1=0.830, weighted-F1=0.819]\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Testing DataLoader 0: 100%|██████████| 32/32 [00:01<00:00, 19.74it/s]\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","       Test metric             DataLoader 0\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","        accuracy            0.7884917855262756\n","        macro-F1            0.7175610065460205\n","        micro-F1            0.8190000057220459\n","        test_loss            0.932634711265564\n","       weighted-F1          0.8095340728759766\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"]}],"source":["_, model, dm = train_model(\"distilbert/distilroberta-base\", \"distilbert/distilroberta-base\")"]},{"cell_type":"markdown","id":"e409d01a-7257-4f9c-a077-cbf6ca0a3b78","metadata":{"id":"e409d01a-7257-4f9c-a077-cbf6ca0a3b78"},"source":["# **Extracting sentence representations (through mean pooling)**"]},{"cell_type":"code","execution_count":null,"id":"48e8c613-4980-45e9-a4c6-2b2c929d79dc","metadata":{"tags":[],"id":"48e8c613-4980-45e9-a4c6-2b2c929d79dc"},"outputs":[],"source":["def extract_representations(model, test_dataloader, pooling_type=\"mean\"):\n","    pooled_output_list, labels_list = [], []\n","\n","    pbar = tqdm(total=len(test_dataloader))\n","    with torch.no_grad():\n","        for _, batch in enumerate(test_dataloader):\n","            attention_mask = batch[1]\n","            labels = batch[2]\n","\n","            _, _, hidden_states, _ = model(batch)\n","            # Extracting the output from last hidden state and attention matrix\n","            # hidden_states = torch.stack(hidden_states)[-1]\n","            # attention_mask = torch.stack(attention_mask)[-1]\n","            hidden_states = torch.stack(hidden_states)[-1]\n","\n","            # Generating the pooled output\n","            if pooling_type == \"mean\":\n","                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","                sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n","                sum_mask = input_mask_expanded.sum(1)\n","                sum_mask = torch.clamp(sum_mask, min=1e-9)\n","                pooled_output = sum_embeddings / sum_mask\n","            elif pooling_type == \"max\":\n","                last_hidden_state = hidden_states\n","                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","                last_hidden_state[input_mask_expanded == 0] = float(\"-inf\")  # Set padding tokens to large negative value\n","                pooled_output = torch.max(last_hidden_state, 1)[0]\n","            else:\n","                # Mean-max pooling\n","                last_hidden_state = hidden_states\n","                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","                sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n","                sum_mask = input_mask_expanded.sum(1)\n","                sum_mask = torch.clamp(sum_mask, min=1e-9)\n","                mean_pooled_output = sum_embeddings / sum_mask\n","                last_hidden_state[input_mask_expanded == 0] = float(\"-inf\")  # Set padding tokens to large negative value\n","                max_pooled_output = torch.max(last_hidden_state, 1)[0]\n","                pooled_output = torch.cat((mean_pooled_output, max_pooled_output), 1)\n","\n","            pooled_output_list.append(pooled_output)\n","            labels_list.append(labels)\n","            pbar.update(1)\n","        pbar.close()\n","\n","    # Concatenate the pooled outputs and labels into tensors\n","    pooled_outputs = torch.cat(pooled_output_list)\n","    labels = torch.cat(labels_list)\n","\n","    return pooled_outputs, labels"]},{"cell_type":"code","execution_count":null,"id":"eea71d2b-a311-433f-b651-4c4403af34d2","metadata":{"tags":[],"id":"eea71d2b-a311-433f-b651-4c4403af34d2","outputId":"f40dcd85-216e-4420-ff89-bc1cf0d77210"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 119/119 [04:28<00:00,  2.26s/it]\n"]}],"source":["train_pooled_outputs, train_labels = extract_representations(model, dm.train_dataloader())"]},{"cell_type":"code","execution_count":null,"id":"fd64b365-52b6-49af-870e-e05e020e9a30","metadata":{"tags":[],"id":"fd64b365-52b6-49af-870e-e05e020e9a30","outputId":"693892b4-741e-41c5-9b83-eaf420b824f1"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 32/32 [01:10<00:00,  2.22s/it]\n"]}],"source":["test_pooled_outputs, test_labels = extract_representations(model, dm.test_dataloader())"]},{"cell_type":"markdown","id":"e095345d-a298-4efc-b626-415053f60774","metadata":{"tags":[],"id":"e095345d-a298-4efc-b626-415053f60774"},"source":["# **Helper functions to generate recall@k, precision@k, and mean average precision@k results**"]},{"cell_type":"code","execution_count":null,"id":"22f22f80-d482-4108-9dda-765d127533ea","metadata":{"tags":[],"id":"22f22f80-d482-4108-9dda-765d127533ea"},"outputs":[],"source":["def recall_at_k(actual, predicted, k=10):\n","    \"\"\"\n","    Computes Recall at k for a set of samples.\n","\n","    Recall at k measures the proportion of relevant items found in the top-k predictions. It's a way to evaluate\n","    how good a model is at retrieving relevant items, considering only the top-k items it has predicted.\n","\n","    Parameters\n","    ----------\n","    actual : list of np.array\n","        A list where each element is an array of correct recommendations for a given sample. These are the items\n","        that are relevant to the user's preferences or needs. Order does not matter in these arrays.\n","\n","    predicted : list of np.array\n","        A list where each element is an array of predicted recommendations for a given sample, ordered by decreasing\n","        confidence. These are the model's top predictions for what the user might prefer or need.\n","\n","    k : int, optional\n","        The number of top predictions to consider when calculating recall. Defaults to 10. This parameter allows\n","        evaluation at different levels of recommendation list length.\n","\n","    Returns\n","    -------\n","    recall_scores : list\n","        A list of recall scores for each sample. Each score is a float between 0 and 1, inclusive, representing\n","        the proportion of relevant items that were included in the top-k predicted recommendations.\n","    \"\"\"\n","    recall_scores = []\n","    for true_labels, predicted_labels in zip(actual, predicted):\n","        num_relevant = len(set(true_labels))  # Count unique relevant items\n","        if num_relevant == 0:  # Check to avoid division by zero if there are no relevant items\n","            recall = 0.0  # If there are no relevant items, recall is undefined; we define it as 0 for practical purposes\n","        else:\n","            # Count how many of the top-k predicted items are relevant\n","            num_retrieved_relevant = len(set(predicted_labels[:k]).intersection(set(true_labels)))\n","            recall = num_retrieved_relevant / float(num_relevant)  # Calculate recall\n","        recall_scores.append(recall)\n","    return recall_scores\n","\n","\n","def precision_at_k(y_true, y_pred, k=10):\n","    \"\"\"\n","    Computes Precision at k for a set of samples.\n","\n","    Precision at k is a measure that calculates the proportion of recommended items in the top-k set that are relevant.\n","    It focuses on the accuracy of the top-k recommendations provided by the model, disregarding the order of\n","    recommendations beyond the scope of k. This metric is useful for evaluating the quality of a recommendation system\n","    where the goal is to present the most relevant items to a user within a limited set of top-k items.\n","\n","    Parameters\n","    ----------\n","    y_true: list of np.array\n","        A list where each element is an array of correct recommendations for a given sample. These represent the items\n","        that are actually relevant to the user. The order of items in these arrays does not matter because precision\n","        at k does not take into account the ranking of the correct recommendations, only their presence within the top k.\n","\n","    y_pred: list of np.array\n","        A list where each element is an array of predicted recommendations for a given sample, ranked by the model's\n","        confidence in those recommendations being relevant. The order of recommendations is crucial here because the\n","        precision at k calculation only considers the relevance of the items in the top k positions of this list.\n","\n","    k: int, optional\n","        The number of top predictions to evaluate against the actual recommendations. Defaults to 10. This parameter\n","        dictates how deep into the list of recommendations the precision calculation will go, effectively setting a\n","        threshold for what is considered a \"top\" recommendation.\n","\n","    Returns\n","    -------\n","    precision_list: list\n","        A list of precision scores for each sample, where each score is a float value representing the proportion of\n","        relevant recommendations found within the top k predictions. The score ranges from 0 to 1, where 0 indicates\n","        no relevant recommendations were found in the top k, and 1 indicates that all top k recommendations were relevant.\n","    \"\"\"\n","    precision_list = []\n","    for index, _ in enumerate(y_true):\n","        intersection = np.intersect1d(y_true[index], y_pred[index][:k])  # Find the common items in actual and predicted top k\n","        precision = len(intersection) / k  # Calculate precision at k\n","        precision_list.append(precision)\n","    return precision_list\n","\n","\n","def apk(actual, predicted, k=10):\n","    \"\"\"\n","    Computes the average precision at k.\n","    This function computes the average prescision at k between two lists of\n","    items.\n","    Parameters\n","    ----------\n","    actual : list\n","             A list of elements that are to be predicted (order doesn't matter)\n","    predicted : list\n","                A list of predicted elements (order does matter)\n","    k : int, optional\n","        The maximum number of predicted elements\n","    Returns\n","    -------\n","    score : double\n","            The average precision at k over the input lists\n","    \"\"\"\n","    if len(predicted)>k:\n","        predicted = predicted[:k]\n","\n","    score = 0.0\n","    num_hits = 0.0\n","\n","    for i,p in enumerate(predicted):\n","        if p in actual and p not in predicted[:i]:\n","            num_hits += 1.0\n","            score += num_hits / (i+1.0)\n","\n","    if not actual:\n","        return 0.0\n","\n","    return score / min(len(actual), k)\n","\n","def mapk(actual, predicted, k=10):\n","    \"\"\"\n","    Computes the mean average precision at k.\n","    This function computes the mean average prescision at k between two lists\n","    of lists of items.\n","    Parameters\n","    ----------\n","    actual : list\n","             A list of lists of elements that are to be predicted\n","             (order doesn't matter in the lists)\n","    predicted : list\n","                A list of lists of predicted elements\n","                (order matters in the lists)\n","    k : int, optional\n","        The maximum number of predicted elements\n","    Returns\n","    -------\n","    score : double\n","            The mean average precision at k over the input lists\n","    \"\"\"\n","    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"]},{"cell_type":"markdown","id":"9ba80139-3d23-416b-9c9c-2355227c476e","metadata":{"id":"9ba80139-3d23-416b-9c9c-2355227c476e"},"source":["# **Retrieval through [FAISS](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/train_pooled_outputs) similarity search**\n","\n","Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n","\n","Faiss is written in C++ with complete wrappers for Python. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at FAIR, the fundamental AI research team of Meta."]},{"cell_type":"code","execution_count":null,"id":"fdf786f3-6f4b-4e57-87f3-02edbf3da330","metadata":{"tags":[],"id":"fdf786f3-6f4b-4e57-87f3-02edbf3da330"},"outputs":[],"source":["def generate_retrieval_results(train_embeddings, test_embeddings, train_labels, test_labels):\n","    \"\"\"\n","    Generates retrieval results for given train and test embeddings and labels,\n","    calculating precision, recall, and mean average precision (MAP) at various levels of k.\n","\n","    Parameters\n","    ----------\n","    train_embeddings : torch.Tensor\n","        Embeddings of the training set items.\n","\n","    test_embeddings : torch.Tensor\n","        Embeddings of the test set items.\n","\n","    train_labels : torch.Tensor\n","        Labels corresponding to the training set embeddings.\n","\n","    test_labels : torch.Tensor\n","        Labels corresponding to the test set embeddings.\n","\n","    Returns\n","    -------\n","    results_df : pandas.DataFrame\n","        DataFrame containing the mean and standard deviation of precision, recall,\n","        and MAP for different values of k.\n","    \"\"\"\n","\n","    # Define the dimensionality of the embeddings and initialize a FAISS index for L2 distance.\n","    dim = train_embeddings.shape[1]\n","    index = faiss.IndexFlatL2(dim)\n","    index.add(train_embeddings.numpy())  # Add train embeddings to the FAISS index for later retrieval.\n","\n","    # Determine the number of neighbors to retrieve with FAISS.\n","    k = 100\n","    D, I = index.search(test_embeddings.numpy(), k)  # Perform the search on the test set embeddings.\n","\n","    # Initialize lists to store the true and predicted labels for each test example.\n","    true_label_list, predicted_label_list = ([] for i in range(2))\n","    for index, rank_indices in enumerate(I):\n","        label = test_labels[index].item()\n","        predicted_label_list.append(train_labels.numpy()[rank_indices])\n","        true_label_list.append(np.array([label] * len(rank_indices)))\n","\n","    # Calculate metrics for different values of k and store results.\n","    results = []\n","    for i in [1, 3, 5, 10, 20, 25, 50, 100]:\n","        # Calculate mean and standard deviation of precision and recall for current k.\n","        mean_precision = np.mean(precision_at_k(true_label_list, predicted_label_list, k=i))\n","        std_precision = np.std(precision_at_k(true_label_list, predicted_label_list, k=i))\n","        mean_recall = np.mean(recall_at_k(true_label_list, predicted_label_list, k=i))\n","        std_recall = np.std(recall_at_k(true_label_list, predicted_label_list, k=i))\n","\n","        # Reset the label lists for MAP calculation.\n","        true_label_list, predicted_label_list = ([] for i in range(2))\n","        for index, rank_indices in enumerate(I):\n","            temp_actual_list = [test_labels[index].item()]\n","            temp_predicted_list = train_labels.numpy()[rank_indices].tolist()\n","\n","            predicted_label_list.append(temp_predicted_list)\n","            true_label_list.append(temp_actual_list)\n","\n","        # Calculate mean and standard deviation of MAP for current k.\n","        mean_map = np.mean([mapk([true], [pred], k=i) for true, pred in zip(true_label_list, predicted_label_list)])\n","        std_map = np.std([apk(a, p, k=i) for a, p in zip(true_label_list, predicted_label_list)])\n","\n","        # Append results for current k to the results list.\n","        results.append({\n","            \"K\": i,\n","            \"Precision Mean\": mean_precision,\n","            \"Precision Std\": std_precision,\n","            \"Recall Mean\": mean_recall,\n","            \"Recall Std\": std_recall,\n","            \"MAP Mean\": mean_map,\n","            \"MAP Std\": std_map\n","        })\n","\n","    # Convert results list to a DataFrame and return.\n","    results_df = pd.DataFrame(results)\n","    return results_df.set_index('K')\n"]},{"cell_type":"code","execution_count":null,"id":"636ba474-79eb-4ce2-9a3b-fdf64a6a4838","metadata":{"tags":[],"id":"636ba474-79eb-4ce2-9a3b-fdf64a6a4838","outputId":"d997c0f3-1687-4ae2-b723-b6339d035b48"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision Mean</th>\n","      <th>Precision Std</th>\n","      <th>Recall Mean</th>\n","      <th>Recall Std</th>\n","      <th>MAP Mean</th>\n","      <th>MAP Std</th>\n","    </tr>\n","    <tr>\n","      <th>K</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.812000</td>\n","      <td>0.390712</td>\n","      <td>0.812</td>\n","      <td>0.390712</td>\n","      <td>0.812000</td>\n","      <td>0.390712</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.280667</td>\n","      <td>0.121580</td>\n","      <td>0.842</td>\n","      <td>0.364741</td>\n","      <td>0.825500</td>\n","      <td>0.369865</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.170000</td>\n","      <td>0.071414</td>\n","      <td>0.850</td>\n","      <td>0.357071</td>\n","      <td>0.827350</td>\n","      <td>0.366298</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.086100</td>\n","      <td>0.034595</td>\n","      <td>0.861</td>\n","      <td>0.345947</td>\n","      <td>0.828852</td>\n","      <td>0.363180</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.044200</td>\n","      <td>0.016011</td>\n","      <td>0.884</td>\n","      <td>0.320225</td>\n","      <td>0.830388</td>\n","      <td>0.359803</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.035520</td>\n","      <td>0.012615</td>\n","      <td>0.888</td>\n","      <td>0.315366</td>\n","      <td>0.830558</td>\n","      <td>0.359419</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>0.018320</td>\n","      <td>0.005548</td>\n","      <td>0.916</td>\n","      <td>0.277388</td>\n","      <td>0.831360</td>\n","      <td>0.357594</td>\n","    </tr>\n","    <tr>\n","      <th>100</th>\n","      <td>0.009370</td>\n","      <td>0.002430</td>\n","      <td>0.937</td>\n","      <td>0.242963</td>\n","      <td>0.831666</td>\n","      <td>0.356889</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Precision Mean  Precision Std  Recall Mean  Recall Std  MAP Mean  \\\n","K                                                                       \n","1          0.812000       0.390712        0.812    0.390712  0.812000   \n","3          0.280667       0.121580        0.842    0.364741  0.825500   \n","5          0.170000       0.071414        0.850    0.357071  0.827350   \n","10         0.086100       0.034595        0.861    0.345947  0.828852   \n","20         0.044200       0.016011        0.884    0.320225  0.830388   \n","25         0.035520       0.012615        0.888    0.315366  0.830558   \n","50         0.018320       0.005548        0.916    0.277388  0.831360   \n","100        0.009370       0.002430        0.937    0.242963  0.831666   \n","\n","      MAP Std  \n","K              \n","1    0.390712  \n","3    0.369865  \n","5    0.366298  \n","10   0.363180  \n","20   0.359803  \n","25   0.359419  \n","50   0.357594  \n","100  0.356889  "]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["generate_retrieval_results(train_pooled_outputs, test_pooled_outputs, train_labels, test_labels)"]},{"cell_type":"code","execution_count":null,"id":"4c9030e2-95d4-4049-9147-0d96b5fa53ad","metadata":{"id":"4c9030e2-95d4-4049-9147-0d96b5fa53ad"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"mix","language":"python","name":"mix"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}